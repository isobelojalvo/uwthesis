%To Do
%%Explain why JEC are used
%%Explain what IRC means
%%Insert figure for Particle Flow
%%Better Explanation for Recoil
%%Lengthen 
\chapter{Event Reconstruction}
\label{chapter:EventReco}
The raw data format 
is used for online reconstruction in the high level trigger
or offline reconstruction. 
In the following section offline event reconstruction is described;
during this stage a number of algorithms are used to identify and
construct the kinematic properties 
of tracks, track vertices,  electrons, muons, taus, jets, missing transverse energy and 
event variables which aid in physics analysis. Where appropriate, configuration
for HLT is indicated. 
\section{Track and Primary Vertex Reconstruction}
%%%%%%%%%%%%%%%%Track Reconstruction [31] %TRK-11-001
%One of the central design motivations to CMS 
%is a multi-layer tracker with a low occupancy, high granularity
%inner silicon pixel detector and a lower granularity silicon strip detector.
Good track reconstruction provides a means for very accurately determining
the momentum of charged particles.
Identification of the primary vertices, signifies a proton-proton interaction in the detector, and 
measurement of the sum of their associated tracks, 
%which the signal originates, 
provides discrimination of the hard scattering process 
from pileup vertices.
Furthermore, using tracker tracks to search for secondary vertices
is crucial for accurate identification of heavy flavored jets.
Tracking and vertexing of charged particles is a crucial part of CMS reconstruction
and physics analysis and is described in the following sections.
\subsection{Track Reconstruction}
\label{sec:TrackReco}
%Track selection involves selection of tracks which are 
%consistent with being produced promptly in the primary 
%interaction region by imposing requirements on the 
%maximum value of significance of the transverse impact 
%parameter relative to the beam spot.
Track reconstruction at CMS follows multiple iterations of a track finding sequence
called the Combined Track Finding (CTF) sequence \cite{CMSTDRPhysics}. 
The CTF sequence consists of seed generation through hit clustering,
track finding via a filtering technique, track trajectory fitting and finally
selection of tracks that pass quality requirements. 
%This technique is known as Combined Track Finding (CTF).
In CTF, the first iteration reconstructs tracks that
have the highest quality. Subsequent iterations reconstruct
tracks with gradually less stringent requirements 
to reconstruct tracks which are lower $p_{t}$, have missing hits or which are greatly displaced.
After each iteration of the CTF track reconstruction sequence, the hits
associated with tracks are removed. This reduces the combinatorial complexity
and simplifies subsequent iterations.% in a search for .

The CTF track reconstruction proceeds as follows:
\begin{itemize}
\item First, seeds are generated which provide the initial track candidates.
Charged particles follow helical paths in the magnetic field. 
Therefore, five parameters (including curvature)
are needed to define a trajectory. To determine these five parameters at least
3 hits (or 2 hits in the pixel detector and a constraint on the origin of the track trajectory from the beam
spot) are required. Seeds are built in the inner part of the tracker
and track candidates are reconstructed outwards. 
The choice to begin seeding in the central region of the tracker and
then move outwards towards the endcaps is due to a finer granularity 
and hence lower occupancy per sensor in the center of the pixel detector. Each iteration of CTF
uses independent quality parameters for seeding layers. 
\item Next, track finding is performed based on the Kalman filter method \cite{KalmanFilter}.
Track finding starts by using the seed trajectories to define and then search
for adjacent layers of the detector with a hit. 
%The next step provides the possibility
%of adding an invalid hit in the case where the particle failed to produce a hit.
After successful identification of adjacent layers with valid hits 
the track finding algorithm updates the trajectories of the tracks. 
\item Track fitting is then performed. Constraints, such as a beam spot matching requirement, 
applied during the track finding stage can introduce bias to the track trajectory. Track
fitting removes bias and provides full information about the track kinematic trajectory\cite{TRK11001}.  
 The trajectory is refitted using a Kalman filter and smoother with
a Runge-Kutta propagator that takes into account both material effects and accommodates
the inhomogeneous magnetic field.
%track selection sets quality flags and discards tracks that fail certain criteria
\item The final step is track selection, where tracks are required to pass a number of quality
based selection criteria. 
The selection criteria is placed on a track's number of layers 
with valid hits, the fit-based $\chi^{2}/dof$, and the track's compatibility
with a primary vertex. In addition to these, several requirements are
imposed as a function of track $p_{T}$, $\eta$, and the number of layers
with valid hits. 
\end{itemize}
%The tracks and primary vertices found with this algorithm are known as pixel 
%tracks and pixel vertices, respectively. 
The performance of the track reconstruction is evaluated in 
$Z\rightarrow \mu\mu$ events using a 'tag and probe'
method whereby a well identified 'tag' muon track is found
and the efficiency of reconstructing a second 'probe' muon is measured; 
the reconstruction efficiency of the 'probe' muon is shown 
in figure \ref{fig:TrackerPerformance}.
\begin{figure}[t]
  \centering
 \begin{subfigure}[b]{.4\textwidth}
	\includegraphics[width=\textwidth]{images/TrackerEffEta.pdf}
  \end{subfigure}
  \begin{subfigure}[b]{.4\textwidth}
	\includegraphics[width=\textwidth]{images/TrackerEffVtx.pdf}
  \end{subfigure}
  	\caption[Tracker Performance]
   	{Tracker efficiency of muons in $Z\rightarrow\mu\mu$ decays. Measured using Tag and Probe as a function of $\eta$ (left) and number of vertices (right)\cite{TRK11001}}
	\label{fig:TrackerPerformance}
\end{figure}
\subsection{Primary Vertex Reconstruction}
Primary vertices (PVs) are reconstructed in order to 
locate and determine the associated uncertainty of each 
proton-proton interaction vertices regardless of whether it is a 'signal' or 'background' vertex. 
Primary vertex reconstruction proceeds in three steps. 
The first step is to select tracks based on their association with a primary interaction region. 
To do this, a number of quality selections are imposed, based upon the significance of the 
transverse impact parameter ($d_{xy}$), the number of strip and pixel hits that are 
associated with a track and the normalized $\chi^{2}$ from the fit to the 
trajectory. In selecting tracks, there is no requirement on the $p_{T}$ of the track; 
this is important so that all PVs including ones from minimum bias events are reconstructed. 
The second step is to cluster the selected tracks based on their $z$ coordinate at their 
point of closest approach to the beam spot. This is done using a Deterministic Annealing (DA) 
algorithm which finds a global minimum given many degrees of freedom \cite{DAAnnealing}. 
The DA algorithm is based upon %fix new and doesn't make sense
an algorithm developed for physical chemistry processes whereby 
a system can be driven to its lowest energy state by a gradual reduction 
of temperature, or annealing. The DA algorithm begins by 
calculating a global $\chi^{2}$ whereby each reconstructed 
track is assigned to a vertex prototype. The assignment of track to vertex is given 
a weight between 0 and 1 based on the compatibility of vertex and track. This assignment 
is controlled by a temperature parameter where an infinite temperature means 
all weights are equal and T=0 means all the weights are 0 or 1. The clustering 
starts with a single prototype vertex at high T and in subsequent iterations 
the prototype vertex is split into multiple vertices. If all tracks are compatible with 
a single vertex then two prototype vertices will become one. Each 
prototype vertex becomes a vertex candidate with all 
associated tracks that have a weight greater than 0.5. 
The third and final step is to take candidate vertices based on DA clustering in $z$ and use
an 'adaptive vertex fitter'  
to compute vertex parameters \cite{KalmanFilter}. These parameters include the 3-D position and  covariance matrix, 
as well as indicators for the success of the fit such as the number of degrees ($n_{dof}$) of freedom for 
each vertex and weights of the track used in each vertex. The adaptive vertex fitter 
uses a modified definition of $n_{dof}$ where, 
\begin{equation} 
n_{dof} = -3+2\sum^{nTracks}_{i=1} w_{i},
\end{equation}
Here, $w_{i}$ is the weight of the $i^{th}$ track. This implies that $n_{dof}$ is strongly 
correlated with the number of tracks that are compatible with arising from the interaction 
region which means that $n_{dof}$ can also be used to select true proton-proton interactions. 
For a vertex to be selected at the analysis level, it is required to have a $z$ position 
smaller than 24 cm with respected to the origin of the detector, a $\rho$ position 
less than 2 cm from the IP and more than 4 degrees of freedom. Of the vertices 
that pass this criteria the signal primary vertex is defined as the one that maximizes 
\begin{equation}
\sum_{i} p_{T,i}^{2},
\end{equation}
where the sum extends over all associated tracks and $p_{T,i}$ is the transverse momentum 
of the $i^{th}$ track.
\section{Electron ID and Reconstruction}
\label{sec:electronReco}
Electrons in CMS are reconstructed using tracker or ECAL seeds. 
Electrons with a low transverse momentum have a higher efficiency 
for reconstruction using tracker seeds; whereas electrons with a
higher transverse momentum have a higher efficiency if reconstructed
using ECAL seeds. When traversing the tracker, electrons pass through 
material equivalent to 0.4-0.8 $X_{0}$ 
this causes them to lose a significant portion of 
energy to radiating bremsstrahlung photons. 
This will in turn cause the electron energy deposit to spread in $\phi$. 
Accurate measurement of electron energy 
at the initial interaction point 
requires collection of photons produced via bremsstrahlung.
%Electron reconstruction is also spread across $\phi$

To begin reconstruction, 
clusters are formed in the ECAL. %, these clusters are then 
%extended in $\phi$ to form super clusters.
Next, electron reconstruction requires
seeds to be produced using either an ECAL driven (1) or a tracker driven approach (2).
(1) is more efficient for electrons with $E_{T}>4$ GeV;
in this method, the clusters produced in the ECAL are
extended in $\phi$ to form super clusters (SCs). 
SCs are then selected and matched back to tracks in the inner tracker layers. 
%The tracker driven approach for electron seeding 
Approach (2) uses hits in the pixel detector to seed
low energy electrons; in this case the energy deposit in the calorimeter
is very broad. All reconstructed tracks are considered as seeds
and the bremsstrahlung hypothesis is tested
by extrapolating a straight line from the track position to the
corresponding ECAL cluster. The process is 
repeated for all layers and a supercluster is defined
by summing all linked electromagnetic cluster deposits. 

Next, trajectories are reconstructed using a dedicated
model of the electron energy loss and fitted using a Gaussian Sum Filter (GSF) algorithm \cite{GSF}. 
For electron reconstruction,
the GSF algorithm is preferred over the typical Kalman Filter algorithm.
This choice is motivated by the fact that bremsstrahlung energy loss, 
as described by Bethe and Heitler, 
does not following a Gaussian distribution. 
So, while the Kalman Filter algorithm is optimal in cases where all probability
densities encountered during track reconstruction are Gaussian,
the GSF algorithm distributions are weighted sums of Gaussians and
appropriately describes electron energy loss.
The electron trajectory builder constructs all possible trajectories
for a given seed and the best fit is chosen using a $\chi^{2}$ approach.
Finally, a trajectory smoother is applied.

To improve the electron selection and reject the large QCD backgrounds in the di-tau analysis a 
Boosted Decision Tree (BDT) based %Ref BDT
discriminator is used. The training has been performed in two
bins of $p_{T}$ and three bins of $\eta$ as shown in Table~\ref{tab:ElectronID-Thresholds}\@.
The BDT has the following $19$ variables as input:
%been trained on data selecting genuine electron candidates from well reconstructed $Z\to ee$ events and
%mis-identified (fake) electron candidates from $Z$+jets events, from which the electrons, which have
%been used to reconstruct the $Z$ boson candidate have been excluded. 

\begin{itemize}
\item
The normalized $\chi^{2}$ of the common track fit, the number of valid hits in the track fit, the normalized
$\chi^{2}$ of the {\it GSFTrack} fit.
\item
The distance in $\eta$ ($\Delta\eta_{SC}({\rm Track}_{vtx})$) and $\phi$ ($\Delta\phi_{SC}({\rm Track}_{vtx})$)
between the reconstructed super cluster in the calorimeter and the track evaluated at the primary vertex
position, the distance in $\eta$ between the super cluster seed and the track evaluated at the calorimeter
surface.
\item
The cluster shape variables $\sigma_{i\eta,i\eta}$ and $\sigma_{i\phi, i\phi}$, where $i\eta$ ($i\phi$)
indicate the integer label of the electromagnetic calorimeter cell in $\eta$ ($\phi$), the cluster shape
variable $f_{e}=1-e1X5/e5X5$, where $e1X5$ ($e5X5$) indicate the energy deposition in an array of $1 \times 5$
($5 \times 5$) cells in the vicinity of the super cluster seed, the cluster shape variable $R9 = e3x3/E_{SC}$,
where $e3x3$ and $E_{SC}$ indicate the energy in an array of $3 \times 3$ cells in the vicinity of the
super cluster seed and the raw energy of the reconstructed super cluster.
\item
The ratio of the hadronic energy over electromagnetic energy of the super cluster ($H/E$), the ratio of
the super cluster energy over the momentum of the associated track evaluated at the selected primary vertex
($E/P$), the variable $1/E_{e}-1/p_{e}$, where $E_{e}$ and $p_{e}$ indicate the reconstructed energy and
momentum of the electron candidate, the ratio of the electron cluster over the momentum of the associated
track and the ratio of the seed cluster over the associated track, where each time the track momentum
has been evaluated at the surface of the calorimeter.
\item
The ratio of the energy that has been reconstructed in the pre-shower detector over the raw energy of
the reconstructed super cluster. The momentum and $\eta$ of the reconstructed electron candidate.
\end{itemize}

An electron is considered as well identified if the BDT discriminator falls above the thresholds shown
in Table~\ref{tab:ElectronID-Thresholds}\@. In addition the electron candidate is required to have a
distance from the selected primary vertex of $d_{z}<0.1$~cm along the $z$ direction of the experiment
 and $d_{0}<0.02 (0.045)$~cm in the plane perpendicular to $z$ in the $e\mu$ ($\mu\tau_{h}$ / $e\tau_{h}$)
decay channel. Furthermore, there should be no missing hits in the inner layers of the pixel detector, no
hits before the selected primary vertex and a vertex fit probability of more than $P>10^{-6}$ to minimize the probability that the electron candidate originates from a photon conversion.

\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|c|c|c|}
%\cline{2-4}                                                                                                                                                                
\multicolumn{1}{c}{ }      & \multicolumn{3}{c}{\bf BDT Discriminator Value ($>$)}                 \\
\cline{2-4}
\multicolumn{1}{c|}{ }     & $|\eta|<0.8$      & $0.8 \leq |\eta| < 1.479$  & $1.479 \leq |\eta|$  \\
\hline
$\pt\leq 20$ GeV          & $0.925$           & $0.915$                    & $0.965$              \\
$\pt>    20$ GeV          & $0.925$ & $0.975$          & $0.985$    \\
\hline
\end{tabular}
\caption{
  Thresholds for the BDT discriminator to identify electrons. For electrons with $\pt>20$ GeV the values in braces correspond to the Tight ID working point.}
\label{tab:ElectronID-Thresholds}
\end{center}
\end{table}

\subsection{Rejection of Electrons from converted photons} %[39]
In the di-tau analysis a non-negligible background contribution 
is due to $\gamma$ + jets production where a high energy
photon converts to an electron-positron pair and a jet is
misidentified as a hadronic tau. To reject this background
electrons from photon conversions are identified and then 
a veto is placed on these electrons.
Photon conversions %ref [39]
are reconstructed by combining opposite sign track pairs and performing
a vertex fit of those tracks to identify conversions.
To reject electrons coming from conversions, a requirement is placed
on the minimum number of hits in the pixel detector given the
track position and direction.
%Electron trigger?? maybe

\section{Muon ID and Reconstruction}
Muons at CMS are reconstructed using information from both the 
tracker and the muon detectors. Muons which come from the decay
of an on shell $W$ boson are typically higher in transverse momentum
than muons which are from tau semi-leptonic decays.
Three muon reconstruction approaches are used: standalone,
global and tracker muon reconstruction. Depending on the 
energy of the muon the reconstructed trajectory is either
solely dependent on the tracker or dependent on the tracker
and the muon system.
%Muon reco [40]
\subsection{Standalone Muon Reconstruction}
Standalone muon reconstruction uses only tracks from the muon system
to reconstruct tracks using a Kalman filter technique which is seeded
by track segments or Level-1 trigger electronics. Tracks are propagated
in iterative steps taking into account the magnetic field, muon energy loss in the material,
multiple scattering and missing hits in the muon system.
Next a suitable $\chi^{2}$ cut is applied to reject bad hits due to showering,
delta rays and pair production. A backward Kalman filter is then applied,
working from outside in and finally the track is extrapolated to the 
beam-spot and a vertex-constrained fit to the track
parameters is performed.
\subsection{Global Muon Reconstruction}
Global muon reconstruction matches standalone 
tracks to tracks in the tracking system. 
Tracks are selected which roughly correspond in momentum and position to 
the standalone muon tracks. This is performed in two steps: First,
tracks are selected in a defined $\eta\times\phi$ region which is centered
on the standalone track. Next, spatial and momentum matching is 
used to select the best matching track. Compatibility of the 
standalone muon track and tracker track with the 
primary vertex is also required. Finally a new 'global track' is created combining
tracker and muon hits; at this stage, no new hits are selected, instead, 
the selected hits are refitted as a global track. If more than one candidate
track pair is matched then the candidate with the best $\chi^{2}$ value
is selected. 
For muons with $p_{T}<200\GeV$ the $p_{T}$ measurement is driven
by the tracker resolution, for muons with $p_{T}>200\GeV$,
the global-muon fit can improve the momentum resolution compared
to the tracker-only fit.
In muons with a higher $p_{T}$, as might be found
in the boosted topologies of $\hbb$ or $\Wbb$ it is useful to require that muons
are globally reconstructed.
\subsection{Tracker Muon Reconstruction}
In tracker muon reconstruction, all tracks with $p_{T}>0.5$ GeV
and $p>2.5$ GeV are considered as possible muon candidates. 
Track reconstruction is outlined in Section \ref{sec:TrackReco}.
After tracks are reconstructed their trajectories are extrapolated to the muon
system. % taking into account effects due to the magnetic field.
If there exists one hit in the muon system which 'matches' the extrapolated track then 
that track is defined as a tracker muon. A 'match' requires
the distance in $x$ between the segment and the extrapolated
track is less than 3 cm or that the pull for $x$ is less than 4. 
The pull is defined as the difference in the position of the matched segment
and the position of the extrapolated track divided by the sum of 
the uncertainties on the position of the matched segment
and the position of the extrapolated track.
%Muon ID (use old muon plots?) mention muon veto in analysis
%Muon trigger
% section on lepton isolation?
\section{Electron and Muon Isolation}
\label{sec:Isolation}
To discriminate signal muons and electrons from leptons
which are created in QCD interactions an isolation requirement is essential. 
%is based on particle-flow candidates, of which all photon 
%and neutral hadron candidates and all charged particle-flow
As pileup of interactions in the detector increases, the performance of standard
combined relative isolation, which sums the 
energy deposited by all PF candidates in a cone of $\Delta R = 0.4$ around the central
lepton and divides the sum by the $p_{T}$ of 
the candidate, degrades. Charged PF particles are associated with 
a vertex using the deterministic annealing algorithm; non charged PF particles
are not associated with any vertex by the vertex algorithm, instead,
they are associated with vertices by using their closest distance in the
$z$ axis after they are extrapolated to the beam line. Using this separation algorithm,
charged particles can be properly associated with a given primary vertex
and used to calculate isolation. However, this algorithm does not
properly account for neutral particles produced in pile-up interactions. 
Therefore, a specific correction, known as a $\Delta \beta$ correction, is
used to account for the neutral energy from other interactions.
A charged particle's transverse momentum sum is created by summing over
the charged particles inside the isolation cone of the lepton while %reread
requiring that those charged particles do not originate from the primary vertex.
The charged particle sum is converted into an expected neutral deposit by
assuming that the average charged to neutral particle ratio is 2:1.
A relative combined isolation variable is then defined as:
\begin{equation}
I_{\rm rel} = \frac{\sum{\pt(\mathrm{charged})} + \max\left(\sum{E_{T}(\mathrm{neutral})} + \sum{E_{T}(\mathrm{photon})} - \Delta\beta, 0\right)}{\pt(\mu \, \, \mathrm{or} \,\, e)}
\label{eq:relIso}
\end{equation}
where $p_{T}(\mathrm{charged})$ corresponds of the $p_{T}$ 
of all charged particle candidates, $p_{T}(\mathrm{photon}$)
and $p_{T}(\mathrm{neutral}$) correspond to the transverse 
energy of the photon and neutral hadron candidates
and $\Delta\beta$ corresponds to the energy estimate of neutral particles due to pile-up.
In the $\mu\tau_{h}$ and $e\tau_{h}$ channels of the MSSM di-tau 
analysis $I_{\mathrm{rel}}<0.1$ is required, both for muons and electrons. In the $\Wbb$
analysis $I_{\mathrm{rel}}<0.12$ is required. 

\section{$\tau$ ID and Reconstruction}
The $\tau$ lepton's high mass means that the $\tau$ plays a very important
roll in the search for the SM higgs boson, and MSSM higgs bosons.
The lifetime of the $\tau$ is short enough that it decays before reaching the inner
most detector. This short lifetime makes $\tau$ reconstruction particularly challenging;
the solution is to reconstruct the decay products of the $\tau$. 
The dominant hadronic $\tau$ decays ($\tau_{h}$) 
and any intermediate resonances are 
outlined in Table \ref{tab:decay_modes}. 
These decays consist of one or three charged $\pi$ mesons and up to two $\pi^{0}$ mesons.
This thesis uses the hadron plus strips (HPS) algorithm for the reconstruction of 
$\tau_{h}$'s. %%%%ref tau id paper
\begin{table}[b]
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
Decay mode & Resonance & Mass (MeV) &  Branching fraction (\%) \\
\hline
$\tau^{-}$  $\rightarrow $  $h^{-} \nu_{\tau}$ &  &  & $11.6\%$ \\
$\tau^{-}$  $\rightarrow $  $h^{-} \pi^{0}  \nu_{\tau}$ & $\rho^{-}$ & 770 & $26.0\%$ \\
$\tau^{-}$  $\rightarrow $  $h^{-} \pi^{0}\pi^{0}  \nu_{\tau}$ & $a_{\rm{1}}^{-}$ & 1200 & $9.5\%$ \\
$\tau^{-}$  $\rightarrow $  $h^{-} h^{+} h^{-} \nu_{\tau}$ & $a_{\rm{1}}^{-}$  & 1200 & $9.8\%$ \\
$\tau^{-}$  $\rightarrow $  $h^{-} h^{+} h^{-}\pi^{0}  \nu_{\tau}$ & & & $4.8\%$ \\
      \hline
\end{tabular}
\caption{
   Branching fractions of dominant hadronic $\tau$ decays and mass of any intermediate resonance. 
   }
\label{tab:decay_modes}
\end{center}
\end{table}

The HPS algorithm 
reconstructs photons into 'strips': these are objects which are built
out of charged particles within a window of size $\Delta\eta=0.05$
and $\Delta\phi=0.20$. The algorithm starts by centering a strip on the
most energetic electromagnetic particle within the PF jet and then it searches
for other electromagnetic particles within the window. If another electromagnetic
particle is found then the object gets associated with the strip and
the four-momentum is recalculated. This procedure is repeated until no other
particles are found. Strips which satisfy the requirement of $p_{T}^{strip}>$ 1 GeV 
are finally combined with the charged hadrons to reconstruct individual
$\tau_{h}$ decay modes \cite{CMS-PAS-TAU-11-001}.

The following decay topologies are considered by the HPS $\tau$ ID algorithm where
h stands for a charged hadron:
\begin{itemize}
\item{\bf Single hadron}
      corresponds to $ h^{-} \nu_{\tau}$
      and $ h^{-} \pi^{0} \nu_{\tau}$ decays
      in which the neutral pions have too little energy to be reconstructed as strips.
\item{\bf One hadron $+$ one strip}
      reconstructs the decay mode $ h^{-} \pi^{0} \nu_{\tau}$
      in events in which the photons from $\pi^{0}$ decay
      are close together on the calorimeter surface.
\item{\bf One hadron $+$ two strips}
      corresponds to the decay mode $ h^{-} \pi^{0} \nu_{\tau}$
      in events in which photons from $\pi^{0}$ decays are well separated.
\item{\bf Three hadrons}
      corresponds to the decay mode $ h^{-} h^{+} h^{-} \nu_{\tau}$.
      The three charged hadrons are required                                                                                                      
      to come from the same secondary vertex.
\end {itemize}

Charged hadrons and strips are required to be contained within a cone
of size $\Delta R=(2.8\GeV/p_{T}^{\tau_{h}})$ where $p_{T}^{\tau_{h}}$ is 
the transverse momentum of the $\tau$ hadron $p_{T}^{\tau_{h}}$ and is 
required to match the $(\eta,\phi)$ direction of the original PF jet within
a radius of $\Delta R=0.1$. 
%%four momenta must match decay modes
%50-200 MeV for pi0, 0.3 to 1.3 GeV for \rho
%and 0.8 to 1.5 GeV for $a_{1}$

%%%%Tau Isolation from paper
%%%Tau performance plots
\subsection{Tau Isolation}
The isolation of $\tau_{had}$ candidates is computed
by summing the transverse momenta of charged particles 
of $p_{T} > 0.5$ GeV plus photons of $E_{T} > 0.5$ GeV 
reconstructed by the PF algorithm
within a cone of size $\Delta R = 0.5$ centered on the $\tau_{had}$.
Charged hadrons considered in the isolation $p_{T}$ sum 
are required to satisfy $\Delta z < 2$ mm with respect to the 
$\tau_{had}$ primary vertex.
Charged hadrons and photons used to build the $\tau_{had}$ candidate 
are excluded from the isolation $p_{T}$ sum.
The contribution of pileup to the $\tau_{had}$ isolation 
is accounted for by applying $\Delta \beta$ corrections:
\begin{equation*}
I_{\tau_{had}} = \sum P_{T}^{charged} (\Delta z < 2\mbox{~mm}) + \max \left( P_{T}^{\gamma} - \Delta \beta, 0 \right).
\end{equation*}
The $\Delta \beta$ corrections are computed by summing the transverse
momenta of charged particles that have a longitudinal impact 
parameters $\Delta z > 2$~mm with respect to the 
$\tau_{had}$ production vertex
and are within a cone of size $\Delta R = 0.8$ around the $\tau_{had}$.
The sum is scaled by a factor $0.4576$, which is chosen to 
make the $\tau_{had}$ identification pileup insensitive:
\begin{equation*}
\Delta \beta = 0.4576 \cdot \sum P_{T}^{charged} (\Delta z > 2\mbox{~mm}).
\end{equation*}

\section{$\mathtt{SVFit}$ Algorithm}
\label{sec:SVFIT}
The tau pair mass, $M_{\tau\tau}$, is reconstructed in a likelihood based algorithm, $\mathtt{SVFit}$\cite{CMS-PAPER-HIG-10-002}.
The kinematics of tau lepton decays is underdetermined by experimental measurements.
%Decays of tau leptons into hadrons are represented by a two--body phase--space model,
%assuming all visible decay products of the tau to constitute one ``particle''.
$5$ parameters are needed to specify hadronic tau decays in this model:
momentum, polar and azimuthal angles of the tau lepton in the laboratory (CMS detector) frame,
plus two decay angles in the rest--frame of the tau lepton.
%In decays of tau leptons to electron or muons two neutrinos are produced
%and the invariant mass of the di--neutrino system constitutes a $6$--th parameter.

These parameters are chosen to be:
\begin{itemize}
\item $x$, the fraction of tau lepton energy (in the laboratory frame) carried by visible decay products.
\item $\phi$, the azimuthal angle of the tau lepton in the laboratory frame.
\item $m_{\nu\nu}$, the mass of the neutrino system.
\end{itemize}
$E_{x}^{miss}$ and $E_{y}^{miss}$, reconstructed in the plane transverse to the beam--axis
provide 2 further constraints.

%albeit of precision that is limited by the experimental resolution on $E_{x}^{miss}$ and $E_{y}^{miss}$,
%amounting to typically $10$~\GeV in the $2011$ and $2012$ data--taking period~\cite{JME_12_002}.

The reconstruction of tau decay kinematics is underconstrained by measured observables;
this is addressed by the likelihood approach implemented in the SVfit algorithm.
%$M_{\tau\tau}$ values are reconstructed by combining the measured observables $E_{x}^{miss}$ and $E_{y}^{miss}$ with a probability model,
%which includes terms for tau decay kinematics and for the \MET resolution.
The model makes a prediction for the probability density $p(\vec{x} \vert \vec{y}, \vec{a})$
to observe the values $\vec{x} = (E_{x}^{miss}, E_{y}^{miss})$ measured in an event,
given that the unknown parameters specifying the kinematics of the tau pair decay have values
$\vec{a} = (x_{1}, \phi_{1}, m_{\nu\nu}^{1}, x_{2}, \phi_{2}, m_{\nu\nu}^{2})$
and the momenta of the visible decay products are equal to the observed $\vec{y} = (p^{vis}_{1}, p^{vis}_{2})$.
The following likelihood model is used to compute probabilities:
\begin{equation}
P(M_{\tau\tau}^{i}) = \int \delta \left( M_{\tau\tau}^{i} - M_{\tau\tau}(\vec{y}, \vec{a} \right) p(\vec{x} \vert \vec{y}, \vec{a}) d\vec{a}
\end{equation}
for a series of mass hypotheses $M_{\tau\tau}^{i}$.
The best estimate $\hat{M}_{\tau\tau}$ for the tau pair mass is taken to be the maximum of $M_{\tau\tau}^{i}$ within the series.
Lower (upper) limits on the reconstructed mass $\hat{M}_{\tau\tau}$ are determined for every event
by the $0.16$ ($0.84$) quantiles of the series of mass hypotheses $M_{\tau\tau}^{i}$
and associated probability values $P(M_{\tau\tau}^{i})$.

\begin{figure}
\setlength{\unitlength}{1mm}
\begin{center}
\begin{picture}(150,78)(0,0)
\put(-5.5, 0.0){\mbox{\includegraphics*[width=78mm]
  {images/svFitPerformance_aLaLorenzo_mutau_visMass.pdf}}}
\put(78.0, 0.0){\mbox{\includegraphics*[width=78mm]
  {images/svFitPerformance_aLaLorenzo_mutau_svFitMass.pdf}}}
\end{picture}
\caption{
         Distribution of visible mass (left) and $M_{\tau\tau}$ reconstructed reconstructed by the SVfit algorithm (right)
         %for simulated $Z \to \tau\tau \to \mu\tau_{had}$ background events
         %and hypothetical MSSM Higgs$ \to \tau\tau \to \mu\tau_{had}$ signals of $m_{A} = 120$, $200$ and $300$~\GeV.
}
\label{fig:expectedSVfitMassDistributions}
\end{center}
\end{figure}


\section{Particle Flow Reconstruction}
In proton-proton collisions, even at energies on the order of a few TeV, 
most stable decay products have a low $p_{T}$.
At CMS, to identify and reconstruct these stable particles a particle flow (PF) event 
reconstruction technique has been developed \cite{PFT09001}\cite{PFT10003}. 
PF event reconstruction combines 
reconstructed tracks with calorimeter deposits
to form object collections of 
electrons, muons, photons, charged and neutral hadrons, HF hadrons and HF EM particles
for each event. These collections can then be used to build taus, Jets, $\MET$
and to quantify lepton isolation and tag jets originating from heavy quarks.

The first step of PF reconstruction is to perform
iterative tracking. Here, tracks are seeded and then reconstructed with tight criteria
where the emphasis is on achieving a low fake rate. 
After a track has been identified its hits in the tracker are removed and successive iterative steps
then loosen track identification criteria.

The second step of PF reconstruction is to produce clusters in the ECAL and the HCAL. 
In this step, cluster seeds are produced from local calorimeter cell energy maxima, 
then topological clusters are grown by combining cells with at least one common side
with a cell already in a cluster. 

The final step in PF reconstruction, is to apply a 'link algorithm' which links
hits in the ECAL, HCAL, tracker and muon system. In the end-cap the link algorithm
begins with the outer most hit in the tracker and extrapolates
first to the 2 layers of the ECAL pre-shower. Next, it searches for topological clusters
in the ECAL barrel and end-cap that correspond to a maximal depth expected of a typical
electron energy deposit profile. A search for topological clusters in the
HCAL is then performed at a depth corresponding to 1 $X_{0}$.
The track is linked to any given depth if the extrapolated position in 
the calorimeter is within cluster boundaries. The cluster can be enlarged by up 
to one cell to account for discontinuities in the detector elements, radiation via
bremsstrahlung or pair production. Finally, a link between a charged-particle
track in the tracker and a muon track in the muon system is established
when a global fit returns an acceptable $\chi^{2}$ value.

After links have been established PF reconstruction is performed which
can be summarized into three steps: First, a collection of electrons is created using GSF filter,
this is further described in section \ref{sec:electronReco}. Electron
 tracks and calorimetric deposits are then removed. Next, 
PF charged hadrons are constructed by identifying links between tracks in the 
tracker and clusters in the ECAL. If the energy deposit in the ECAL is 
the same as the total $p_{T}$ in the tracker within calorimetric uncertainty a 
PF charged hadron is created. If the energy in the calorimeters is much higher
than a PF photon or a PF neutral hadron might also be created. If the energy in 
the calorimeters is too small then a relaxed search for hits in the muon system
is performed. Finally, remaining clusters of ECAL and HCAL clusters give rise
to PF photons and PF neutral hadrons. Figure \ref{fig:ParticleFlowJet} %insert PFjet figure
shows relative fractions of charged hadrons, photons, neutral hadrons, electrons,
hf hadrons and em particles in jets.

%\subsection{Particle Flow Jet Reconstruction}
\begin{figure}[b]
  \centering
	\includegraphics[width=0.5\textwidth]{images/ParticleFlowJet.png}
  	\caption[PF Jet]
   	{Particle Flow jet composition}
	\label{fig:ParticleFlowJet}
\end{figure}

\section{Jet ID and Reconstruction}

%Anti-Kt algo [43]
Jet identification and reconstruction is of central importance to the understanding
and identification of many physics processes. Furthermore, efficient b jet 
identification is of central importance
in the measurement of the $\Wbb$ cross section.  
In the search for an MSSM higgs boson, production in association with b jets
is enhanced. Therefore, efficient jet and b jet identification is crucial.

\subsection{Jet Reconstruction}
\label{sec:JetReco}
\begin{figure}[ht]
  \centering
	\includegraphics[width=0.8\textwidth]{images/AntiKTAlgo.png}
  	\caption[Comparison of Jet Clustering Algorithms]
   	{Comparison of Jet Clustering Algorithms \cite{Cacciari:2008gp}}
	\label{fig:antiKt}
\end{figure}

Jet clustering algorithms have become a crucial part of physics analysis.
Jet clustering in this thesis is performed using 
the anti-$k_{T}$ jet clustering algorithm \cite{Cacciari:2008gp}.
In order to perform 
comparisons with perturbative effects in theoretical calculations jet clustering algorithm
must be both infrared and collinear (IRC) safe. 
When applied at particle level, an algorithm is collinear safe if the hardest particle
will not be easily changed by the presence of another collinear particle. %check/expand
Two illustrate infrared safety, consider
 two hard particles on opposite edges of a jet cone with no particles between them; in this case
two cones are formed, each centered on a hard particle. An algorithm is infrared unsafe
if the added presence of a soft particle between the two hard particles causes a third jet 
to be formed. The difference with and without the soft particle results in non-cancellation
of divergent real soft production and corresponding virtual contributions \cite{InfraredUnsafe}.
A jet clustering algorithm is infrared unsafe if the number of jets created through clustering
increases when including an additional soft particle within the cone of a previously formed jet. 
A jet clustering algorithm is considered collinear safe if a hard particle is able to split 
collinearly and that splitting does not influence the shape of stable jet cone \cite{CollinearSafe}. 
The development and subsequent choice of the anti-$k_{T}$ jet clustering algorithm
was stimulated by questions of sensitivity to non-perturbative effects like hadronization
and underlying event contamination. Previously used jet clustering algorithms
such as the $k_{T}$ \cite{ktAlgo} %ref 1
and Cambridge/Aachen \cite{cambAachen} %ref 2
jet clustering algorithms were IRC safe, however, they
had the property that soft radiation could provoke irregularities in the boundaries
of final jets. 
Algorithms such as SIScone \cite{SISCone} %ref 4
that were soft-resilient were not IRC safe.
%%%%%%%%%%%%%%Paused editting here Dec 28 5:54pm
To perform jet clustering in the anti-$k_{T}$, $k_{T}$ and Cambridge/Aachen jet algorithm
a distance $d_{ij}$ is introduced between PF particles $i$ and $j$ and $d_{iB}$ between
the particle entity and the beam (B). The clustering proceeds by identifying the smallest distances
between candidate particles in the cluster.
If the minimum distance is $d_{ij}$ the entities $i$ and $j$ are combined. 
When the minimum distance is $d_{iB}$ then $i$ 
is considered a jet and all its entities in the jet are removed from the list of PF particles. The procedure
is repeated until no entities are left. The definition of $d_{ij}$ and $d_{iB}$ are,
\begin{equation}
d_{ij}=\mathrm{min}(k_{ti}^{2p},k_{tj}^{2p})\frac{\Delta^{2}_{ij}}{R^{2}}
\end{equation}
\begin{equation}
d_{iB}=k_{ti}^{2p}
\end{equation}
where $\Delta_{ij}^{2}=(y_{i}-y_{j})^{2}+(\phi_{i}-\phi_{j})^{2}$, $k_{ti}$ is the 
transverse momentum, $y_{i}$ is the rapidity and $\phi_{i}$ is the azimuthal angle of 
the particle $i$. 
The case where $p=1$ is the $k_{T}$ algorithm, $p=0$ corresponds to the Cambridge/Aachen
algorithm and $p=-1$ is the anti-$k_{T}$ algorithm.
The effects of each of these algorithms on an event with a few well-separated hard particles
and many soft particles is shown in figure \ref{fig:antiKt}.
As can be seen in the figure, the key feature of the anti-$k_{T}$ algorithm is 
that soft particle do not modify the shape of the jet while hard particles do.
The anti-$k_{T}$ clustered jets in this thesis use R=0.5. 
%

%Jet energy corrections [44]
\subsection{Jet Energy Corrections}
Jet energy corrections (JEC)
are applied to improve the accuracy of the jet $p_{T}$ measurement
and to flatten the jet energy response as a function
of~$\eta$ and $p_{T}$~\cite{jme-10-010}.
JEC are applied by taking the dot product of a vector-factor, $C$, to each component of the jet four-momentum,
\begin{equation}
p_{\mu}^{cor}=C\cdot p_{\mu}^{raw}
\end{equation}
where $p_{\mu}^{cor}$ is the corrected jet four-momentum vector and
$p_{\mu}^{raw}$ is the raw jet four-momentum\cite{ApplyJEC}.
$C$ itself is composed of an offset correction $C_{offset}$, the MC
calibration factor $C_{MC}$, and the residual calibrations $C_{rel}$ and $C_{abs}$
for the relative and absolute energy scales. The correction $C_{offset}$ 
removes the extra energy due to noise and pile-up, factor $C_{MC}$ removes
the non-uniformity in $\eta$ and the non-linearity in $p_{T}$ and the
residual corrections account for the small differences between data and simulation. These
are combined into the overall correction factor $C$ by,
\begin{equation}
C=C_{offset}(p_{T}^{raw})\cdot C_{MC}(p_{T}',\eta)\cdot C_{rel}(\eta)\cdot C_{abs}(p_{T}'').
\end{equation}
Here, $p_{T}'$ is the jet $p_{T}$ after applying the offset correction and  
$p_{T}''$ is the jet $p_{T}$ after applying all previous corrections.
Jets are required to pass identification
criteria that eliminate jets originating or being seeded by
noisy channels in the calorimeter~\cite{Chatrchyan:2009hy}.
Jets which fall within $\Delta R < 0.5$ from a lepton candidate
are not included in the jet collection.

%Pileup ID
\section{b-Jet ID and Secondary Vertices}
\label{sec:CSVAlgo}
The Combined Secondary Vertex (CSV) b-tagging algorithm makes use
of the long lifetime and heavy-flavor of b-hadrons.
%for both b-jets, it is also required that both b-Jets have atleast one secondary vertex.                                                                                   
The CSV b-tagging algorithm combines the following variables into a single discriminating
variable using a likelihood ratio technique: secondary vertex mass, multiplicity of charged
particles associated to the secondary vertex, the flight significance associated to the
secondary vertex, the energy of charged particles associated to the SV divided by the energy
of all charged particles associated to the jet, the rapidities of charged particle tracks associated
to the secondary vertex, and the track impact parameter significance exceeding the charm threshold.
This algorithm was tuned
using b, c and non-heavy flavour jets from QCD and top samples and provides extreme discrimination
of heavy and udsg jets. At a b-tagging efficiency of 60$\%$, jets from light quarks can be reduced
by a factor of 100!~\cite{refCSV}.

Secondary vertices are reconstructed inside the jet using the Trimmed Kalman Vertex Finder \cite{KalmanFilter}.
This algorithm begins by taking as input all tracks that are inside of the jet and performing a compatibility fit. It then removes the least compatible track and refits the vertex; this procedure is repeated until
the fit is below a given threshold. The secondary vertex studied in this thesis is that which has
the greatest significance of flight distance.

\section{Missing Transverse Energy, Recoil Corrections and MVA $\MET$}
\label{sec:MET}

Missing transverse energy is defined using PF candidates as,
\begin{equation}%%%add vectors above everywhere
\MET = -\sum_{i} p_{T}
\end{equation}
where $i$ runs over all reconstructed PF candidates. 
To improve the $\MET$ resolution in 
events with jets and neutrinos in the decay products 
which are expected to have a high $\MET$, such as $\Wbb$,
recoil corrections are applied to the $\MET$. 

To improve the $\MET$ resolution a correction to the recoil of the generated bosons is applied. 
Momentum conservation in the transverse plane requires,
\begin{equation}
\MET+q_{T}+u_{T}=0
\label{eq:recoilEq}
\end{equation}
%where $\MET$ is the missing transverse energy,
where $q_{T}$ is the vector boson transverse momentum which is measured
in $Z\rightarrow\mu\mu$ data and matched to simulation in $W\rightarrow\mu\nu$.
Finally, $u_{T}$ is the transverse momentum of the hadronic recoil.
\begin{equation}
u_{T}\equiv\sum_{j} p_{j,T},
\end{equation}
where the index $j$ runs over all particles initiating at the interaction point excluding
the vector boson. 

Multivariate Analysis (MVA) 
$\MET$ is used in the search for a MSSM $h\rightarrow\tau\tau$.
MVA $\MET$ aims at improving further the $\MET$ and makes use of a series of multivariate regressions
based on a boosted decision tree (BDT) \cite{BDT}.
%A decision tree is trained on dedicated signal and background samples 
A decision tree requires as input both a signal and background monte carlo sample.
These samples are split into two parts: one part is used for training (creation of the decision tree)
the second part is used for testing (to test the final classifier after training).
Using the training sample, a decision tree is created by defining a number 
of discriminatory variables from
an event and then splitting each variable by some value which optimizes
the separation of signal and background. 
If the final 
variable selection has a purity of signal over signal plus background above a given threshold
then it is considered a 'leaf' and used in the selection of a signal and background events.
A score of 1 is assigned if the event falls on a signal leaf and 0 if it falls on a background leaf.  
Boosting is then performed by taking all signal events that were misclassified as
background or background events misclassified as signal and increasing the weight of
that event (boosting). A new tree is built using the new weights and the final score
is calculated using these weights. A similar method is used to determine a BDT regression
scale factor. 

%through use of recoil corrections and an MVA which targets the true value of the $\MET$.
%These procedures are described in the next sections.
%\subsection{Recoil Correction}
%\subsection{MVA MET}
%In particular, MVA $\MET$ improves measurement of the $\MET$
%in the presence of high pileup.
The MVA $\MET$ uses the BDT regression model 
to determine correction factors to $u_{T}$. This correction
is performed in two steps: %using two bdts
first, compute a correction to the azimuthal angle
of $u_{T}$ by training a BDT with the true hadronic recoil, $-q_{T}$, as the target. Next
 a separate BDT is trained to predict the magnitude of $u_{T}$. This corrected
 $u_{T}$ is then used in equation \ref{eq:recoilEq} %%%cite equation with recoil
 to calculate a new MVA $\MET$. %list variables? Do I care? it's an f*ing BDT.
